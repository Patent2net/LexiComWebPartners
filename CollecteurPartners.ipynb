{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CollecteurPartners.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFpKPmeOUPS5"
      },
      "source": [
        "# Scrapper de sites\n",
        "\n",
        "> Récupère le contenu de chacun des URL classé par typologie (variable ndf dans le code), chargé depuis le fichier Sites.json.\n",
        "\n",
        "> Stocke le résultat dans ndf qui conditionne la variable \"fichier de sortie\". Celle-ci peut être adaptée pour pointer sur une zone correcte de votre drive dans la zone \"Personnalisation\" dénotée ci-dessous. Les données des sites collectés sont stockées au format pickle dans le dossier /OUT/ContenusSites classé par typologie selon Sites.json.\n",
        "\n",
        "> La variable de sortie est de type dictionnaire pour conserver les url sources (flexibilité pour pouvoir corriger). Cf. commentaires infra. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0A5eMDAyoOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0716c0c-b398-489d-90e9-392e7b43571c"
      },
      "source": [
        "# Il faut initialiser le dossier à partir de GitHub\n",
        "# cette action est à faire une seule fois lors de la première utilisation\n",
        "!git clone https://github.com/Patent2net/LexiComWebPartners.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LexiComWebPartners'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 29 (delta 8), reused 27 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (29/29), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Patent2net/LexiComWeb.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8pJlzgEbvLM",
        "outputId": "fd99116e-a4bd-4ebe-9a57-999f6cf03b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LexiComWeb'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 76 (delta 11), reused 73 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (76/76), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra410KOWyoyR"
      },
      "source": [
        "stockageEntree = \"/content/LexiComWeb/\"\n",
        "stockageEntreePartners = \"/content/LexiComWebPartners/\"\n",
        "stockageSortie = \"/content/LexiComWebPartners/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKq-FnXyg7Pg"
      },
      "source": [
        "import os, urllib\n",
        "os.chdir('/content/LexiComWeb/RESSOURCES/')\n",
        "from outils import isPartner, text_from_html, myRequest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awgenByYki0v"
      },
      "source": [
        "\n",
        "---\n",
        "# Personnalisation (Option)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqKP7XYOki0w"
      },
      "source": [
        "> Ces cellules permettent de réaliser les traitements à partir de son propre espace de stockage. A n'exécuter que dans ce cas en adaptant les dossiers d'entrée et sortie. NE PAS EXECUTER SAUF A VOULOIR PERSONNALISER LES TRAITEMENTS.\n",
        "\n",
        "Si vous avez exécuté le scraper et reconstitué l'ensemble des dossiers nécessaires sur votre drive, ces cellules vous permettent de configurer ce notebook pour travailler sur vos données et non celles issues du git (accessibles sur un dossier virtuel via le menu *Fichiers* à gauche)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr3_QQJq_veT",
        "outputId": "05ae10bd-4e5b-417d-ad5f-a6e22fe81cb2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpnXg8eWHorz"
      },
      "source": [
        "# Récupérer les entrées sur son drive\n",
        "stockageEntree = \"/content/drive/MyDrive/OUT\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ozLFiEG_w5H"
      },
      "source": [
        "# Récupérer les sorties sur son drive (créer un dossier \"OUT\")\n",
        "stockageSortie = \"/content/drive/MyDrive/OUT\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbKLKuPg_0jY"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvC9l2YvRmP9"
      },
      "source": [
        "# recup import requests, re, pickle\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib import parse \n",
        "import requests\n",
        "import re\n",
        "import json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Le fichier niveau1-PNPC.csv a été réalisé avec Hyphe pour collecter depuis le site du PNPC (http://www.portcros-parcnational.fr/fr) l'ensemble des liens sortants. Le site pointé par chacun de ces liens est récursivement collecté jusqu'à un niveau de profondeur 3 pour déterminer l'existence d'un lien retour vers le PNPC. La liste des urls de ce fichier consigne ces adresses de sites considérés alors comme des partenaires de la communication Web.\n",
        "\n"
      ],
      "metadata": {
        "id": "nUkhdFN7nOWF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwbNB1nPgycs"
      },
      "source": [
        "with open (stockageEntreePartners + \"RESSOURCES/NIVEAU1-PNPC.csv\", \"r\") as partners:\n",
        "  donnee= partners.readlines()\n",
        "partenaires = []\n",
        "for lig in donnee[1:]:\n",
        "  urls = lig.split(\";\")[2] # 3e colonne\n",
        "  partenaires.extend(urls.split()) # liste d'URLs séparés par des espaces"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Hyphe récupère une liste de partenaires (urls) que le logiciel consigne selon plusieurs protocoles. Quelquefois le chemin est vide (racine du site) d'autres non... Pour pouvoir tester l'appartenance d'un site à cette liste nous normalisons ces urls et l'étendons pour satisfaire la variété du web. Nous retenons ici qu'un site web peut être identifié par les urls sous les formes suivantes (deux protocoles avec ou sans 'www' dans la dénomination) :\n",
        "\n",
        "*   http(s)://site.extension\n",
        "*   http(s)://www.site.extension\n",
        "\n",
        "La variable partenairesPropres sert à les extrapoler à partir des données de Hyphe.\n",
        "\n"
      ],
      "metadata": {
        "id": "_rMlfMeIxnRG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJocrXCkJGdl"
      },
      "source": [
        "partenairesPropres = []\n",
        "for url in set(partenaires):\n",
        "  url = url.strip()\n",
        "  url = url.replace('\"', \"\")\n",
        "\n",
        "  urlP = parse.urlparse(url)\n",
        "  if 'https' in urlP.scheme:\n",
        "    if 'www.' in urlP.hostname:\n",
        "      partenairesPropres.append(urlP.scheme + '://' + urlP.hostname)\n",
        "      partenairesPropres.append(urlP.scheme + '://' + urlP.hostname.replace('www.', ''))\n",
        "      partenairesPropres.append('http://' + urlP.hostname.replace('www.', ''))\n",
        "      partenairesPropres.append('http://' + urlP.hostname)\n",
        "\n",
        "    else:\n",
        "      partenairesPropres.append(urlP.scheme + '://www.' + urlP.hostname)\n",
        "      partenairesPropres.append(urlP.scheme + '://' + urlP.hostname)\n",
        "      partenairesPropres.append('http://www.' + urlP.hostname)\n",
        "      partenairesPropres.append('http://' + urlP.hostname)\n",
        "  else:\n",
        "    if 'www.' in urlP.hostname:\n",
        "      partenairesPropres.append(urlP.scheme + '://' + urlP.hostname)\n",
        "      partenairesPropres.append(urlP.scheme + '://' + urlP.hostname.replace('www.', ''))\n",
        "      partenairesPropres.append('https://' + urlP.hostname.replace('www.', ''))\n",
        "      partenairesPropres.append('https://' + urlP.hostname)\n",
        "\n",
        "    else:\n",
        "      partenairesPropres.append(urlP.scheme + '://www.' + urlP.hostname)\n",
        "      partenairesPropres.append(urlP.scheme + '://' + urlP.hostname)\n",
        "      partenairesPropres.append('https://www.' + urlP.hostname)\n",
        "      partenairesPropres.append('https://' + urlP.hostname)\n",
        "\n",
        "partenairesPropres = list(set(partenairesPropres)) # sans doublons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzeyIVB-ekt_",
        "outputId": "d42a24eb-1dae-4520-9d59-b9ceafccd28f"
      },
      "source": [
        "print(len(set(partenairesPropres)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIPeBmTGgdZd"
      },
      "source": [
        "> recup des déjà collectés"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q0AnATygcQB"
      },
      "source": [
        "fichierDeSortie = stockageEntree + '/ContenusPartners/' + 'Partners.json'\n",
        "with open (fichierDeSortie, 'r', encoding ='utf8') as  fictemp: #\n",
        "  listePartners = json.load(fictemp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2cub-Bqj4YK"
      },
      "source": [
        "import urllib3\n",
        "\n",
        "urllib3.disable_warnings() # pourquoi les certificats SSL ne passent pas aujourd'hui ? 30/11 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zxsE3AeBrCd"
      },
      "source": [
        "# Première boucle pour lever les problèmes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhM4fmUohayD"
      },
      "source": [
        "> La dynamique du web fait que certains sites peuvent ne pas être accessibles, ne pas répondre à un instant t, ou faire planter le collecteur. Ce qui suit teste chaque URL et construit la variable BadUrl avec les urls en erreur.\n",
        "\n",
        "La liste des url collectés et étendue par le procédé précédent génère aussi des adresses pas forcément valides. Ce qui suit est fait pour les expurger."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "listePartners = dict()"
      ],
      "metadata": {
        "id": "mW7rfc3nmRhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZUYVvJjWKIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f961aad6-f2bd-4f59-fea1-6bc8b5993289"
      },
      "source": [
        "BadUrl = []\n",
        "Done = []\n",
        "for url in partenaires: \n",
        "  if url not in listePartners .keys() and isPartner(url, partenairesPropres+partenaires):\n",
        "      try:\n",
        "        webpage = myRequest(url)\n",
        "      except:\n",
        "        print(\"bad\", url)\n",
        "        BadUrl.append(url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bad http://www.fr-fr.facebook.com/pages\n",
            "bad https://www.fr-fr.facebook.com/pages\n",
            "bad https://alizee-soft.com\n",
            "bad https://www.alizee-soft.com\n",
            "bad https://port-cros-parcnational.fr\n",
            "bad https://www.port-cros-parcnational.fr\n",
            "bad http://www.espritparcnationalportcros.com\n",
            "bad https://www.espritparcnationalportcros.com\n",
            "bad http://var.gouv.fr\n",
            "bad https://var.gouv.fr\n",
            "bad https://www.var.gouv.fr\n",
            "bad https://www.festival-galathea.com\n",
            "bad https://bateaux-taxi.com\n",
            "bad http://obi1-portcros-parcnational.fr\n",
            "bad http://www.obi1-portcros-parcnational.fr\n",
            "bad https://obi1-portcros-parcnational.fr\n",
            "bad https://www.obi1-portcros-parcnational.fr\n",
            "bad http://wixsite.com\n",
            "bad https://wixsite.com\n",
            "bad http://marches-publics.gouv.fr\n",
            "bad http://www.marches-publics.gouv.fr\n",
            "bad https://marches-publics.gouv.fr\n",
            "bad http://univ-tln.fr\n",
            "bad https://univ-tln.fr\n",
            "bad https://cbnmed.fr\n",
            "bad https://www.cbnmed.fr\n",
            "bad http://superdoc.com\n",
            "bad http://www.superdoc.com\n",
            "bad https://superdoc.com\n",
            "bad https://www.superdoc.com\n",
            "bad http://www.parc-pyrenees.com\n",
            "bad https://www.parc-pyrenees.com\n",
            "bad http://mersansplastique\n",
            "bad http://www.mersansplastique\n",
            "bad https://mersansplastique\n",
            "bad https://www.mersansplastique\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUPIAATTEjiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844766c5-7573-4ccb-cafd-51fecca802d8"
      },
      "source": [
        "print (len(BadUrl ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjiqoIqHupFF"
      },
      "source": [
        "done = [] \n",
        "listePartners = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNQngfXfRIGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dbf43e2-2ff5-4cbb-8884-340217509719"
      },
      "source": [
        "for url in partenaires: \n",
        "  webpage =\"\" \n",
        "  if url not in listePartners .keys() and isPartner(url, partenairesPropres):\n",
        "    if url not in BadUrl and url not in done:\n",
        "      #soup = dict() # changement de type de données pour conserver l'URL source\n",
        "  # récupère l'URL d'un site web et enregistre la page web\n",
        "      del(webpage)\n",
        "      try:\n",
        "        webpage = myRequest(url)\n",
        "      except:\n",
        "        BadUrl .append(url)\n",
        "        urlTemp = url\n",
        "        url = url .replace('http:', \"https:\")\n",
        "        print (\"test https \", url)\n",
        "        try:\n",
        "          webpage = myRequest(url)\n",
        "        except:\n",
        "          BadUrl .append(url)\n",
        "          if 'www' not in url:\n",
        "            url = url .replace('https://', \"https://www.\")\n",
        "          try:\n",
        "            webpage = myRequest(url)\n",
        "            print (\"sans www ?\")\n",
        "          except:\n",
        "              print (\"rien à faire \", url)\n",
        "              BadUrl .append(url)\n",
        "              continue\n",
        "  # récupère le contenu de la page web à l'aide de BeautifulSoup\n",
        "    #soup.append(BeautifulSoup(webpage.content, \"html.parser\"))\n",
        "      tempoSoup = BeautifulSoup(webpage.content, \"html.parser\")\n",
        "      if tempoSoup.title is not None:\n",
        "        titre = tempoSoup.title.text \n",
        "      else:\n",
        "        titre = \"\"\n",
        "      if tempoSoup.description is not None:\n",
        "        desc = tempoSoup.description.text \n",
        "      else:\n",
        "        desc = \"\"\n",
        "      texte = text_from_html(webpage.content)\n",
        "      if len(titre)>0 and len(desc)>0:\n",
        "        listePartners[url] = titre + '\\n' + desc + \"\\n\" + texte\n",
        "      elif len(titre)>0:\n",
        "        listePartners[url] = titre + \"\\n\" + texte\n",
        "      elif len(desc)>0:\n",
        "        listePartners[url] = desc + \"\\n\" + texte\n",
        "      else:\n",
        "        listePartners[url] = texte\n",
        "      # le contenu est dans le \"casier\" nommé par l'url\n",
        "    # nettoyage\n",
        "    \n",
        "    # soup [cle] = re.sub('^a-zA-Z0-9àâäèéêëîïôœùûüÿçÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ', ' ', soup [cle])\n",
        "      re.sub('^a-zA-ZàâäèéêëîïôœùûüÿçÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ', ' ', listePartners [url])\n",
        "      listePartners[url] = listePartners[url].replace(\"\\xa0\", \" \")\n",
        "      listePartners[url] = listePartners[url].replace(\"\\n\", \" \")\n",
        "      listePartners[url] = listePartners[url].replace(\"’\", \"'\")\n",
        "      listePartners[url] = listePartners[url].translate('utf8')\n",
        "      dictionary = {\"\\\\\": \"\"}\n",
        "      transtable= listePartners[url].maketrans(dictionary)\n",
        "      listePartners[url] = listePartners[url].translate(transtable)\n",
        "      listePartners[url] = str(listePartners[url])\n",
        "      \n",
        "    # c'est pas tip top. Certains caractères restent. Variable selon les sites. \n",
        "\n",
        "    # sauvegarde\n",
        "      fichierDeSortie = stockageSortie + '/ContenusPartners/Partners.json'\n",
        "      with open (fichierDeSortie, 'w') as  fictemp: #on met tous les contenus dans un pickle\n",
        "        json.dump(listePartners, fictemp)\n",
        "    done .append (url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test https  https://risque-prevention-incendie.fr\n",
            "rien à faire  https://www.risque-prevention-incendie.fr\n",
            "test https  https://www.risque-prevention-incendie.fr\n",
            "rien à faire  https://www.risque-prevention-incendie.fr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQXr0XChssso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "427ef7be-2e1c-4933-a36e-6149996ba2e7"
      },
      "source": [
        "len(set(listePartners.values()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQm5mqYgVeGj"
      },
      "source": [
        "## Commentaires et exemple de ce qui est récupéré\n",
        "\n",
        "\n",
        "*  Les clés du dictionnaire de sortie sont les url\n",
        "*  Le contenu du dictionnaire pour une clé donne le texte récupéré\n",
        "*  Il peut y avoir des doublons de contenu (cas des sites qui répondent à deux URL équivalente (avec et sans www par ex.).\n",
        "* Ce dernier point sera traité dans l'instrument suivant (TraiteContenuPartners.ipynb)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gn7V-QWTyQV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "60c59fc0-c270-421f-9f5f-a9a0f87f8503"
      },
      "source": [
        "listePartners [url]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Accueil - Sauvegarde des forets varoises               A propos   Qui sommes-nous  Découvrir le projet COPAINS    Nos services   Espaces naturels  Espaces verts  Propreté urbaine  Nettoyage des plages    Nos produits bio   Tous nos produits  Où trouver nos produits ?  Le panier Porquerollais  Conditions de Livraison    Agenda  Devenir partenaire   Particuliers  Entreprises  Nos partenaires    Contact  0 Article           Sélectionner une page                             Sauvegarde Des Forêts Varoises Sauvegarde des Forêts Varoises,  acteur local de l'environnement au service de l'emploi et du développement des territoires depuis 1991.         Pour la préservation de l'environnement et l'inclusion dans l'emploi             Une Economie Sociale et Solidaire au service du  développement des territoires et de l'emploi   Nous sommes engagés sur des actions de préservation et de valorisation de notre environnement dans le cadre d'un projet solidaire qui soutient l'insertion par l'emploi.     En savoir plus       Le projet COPAINS sur l'Ile de Porquerolles   Nous pratiquons une agriculture durable  en cœur de Parc national, à Porquerolles, pour le maintien de la vocation agricole de l'île nourricière et pour la préservation des collections variétales de figuiers, oliviers et mûriers.   En savoir plus             Nos services               Entretien  des espaces  naturels           Entretien  espaces  verts            Propreté  urbaine                         Nettoyage  de plages       Agenda des foires et marchés       Point de vente Porquerolles 14/12/2021 à 10 h 00 min 13 h 00 min Point de vente Porquerolles 07/12/2021 à 10 h 00 min 13 h 00 min   Voir tous les évènements       Découvrez nos produits         Adopter un figuier à Porquerolles  55,00 €  TTC    Adopter un mûrier à Porquerolles  70,00 €  TTC    Caviar bio d'aubergines au curry de Porquerolles  5,50 €  TTC    Caviar bio d'aubergines de Porquerolles  5,50 €  TTC    Caviar bio d'aubergines de Porquerolles  3,50 €  TTC    Chutney bio de figues de Porquerolles  4,00 €  TTC    Chutney bio de figues de Porquerolles  7,00 €  TTC    Confit d'oignons bio de Porquerolles  4,50 €  TTC            Tous nos produits                     Adresse  363 Chemin de l'Estanci – Giens 83400 Hyères les Palmiers     Contactez-nous  04 94 58 96 69 contact@asdfv83.com     Suivez-nous   Suivre       Liens utiles  Mentions légales  CGV Nos partenaires                                         \""
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}